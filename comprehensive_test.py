#!/usr/bin/env python3
"""
Comprehensive Test Suite for AMR Semantic Parsing
=================================================

This script tests all major components of the AMR system.
"""

import sys
import json
from pathlib import Path

# Add src to path
sys.path.insert(0, 'src')

def test_data_processing():
    """Test data processing functionality."""
    print("\n" + "="*60)
    print("üìä Testing Data Processing")
    print("="*60)
    
    try:
        from src.data_processing import AMRProcessor, DataLoader
        
        # Test AMRProcessor
        processor = AMRProcessor()
        print("‚úÖ AMRProcessor initialized")
        
        # Test DataLoader
        loader = DataLoader()
        print("‚úÖ DataLoader initialized")
        
        # Test loading processed data
        if Path("data/processed/train.jsonl").exists():
            data = loader.load_jsonl("data/processed/train.jsonl")
            stats = loader.get_data_statistics(data)
            
            print(f"‚úÖ Loaded {len(data)} training samples")
            print(f"   Avg input length: {stats['avg_input_length']:.1f}")
            print(f"   Avg output length: {stats['avg_output_length']:.1f}")
            print(f"   Max input length: {stats['max_input_length']}")
            print(f"   Max output length: {stats['max_output_length']}")
            
            # Test data splitting
            from src.data_processing.data_loader import DataSplit
            split_config = DataSplit(train_ratio=0.8, val_ratio=0.1, test_ratio=0.1)
            split_data = loader.split_data(data[:20], split_config)  # Test with small subset
            
            print(f"‚úÖ Data splitting works:")
            for split_name, split_samples in split_data.items():
                print(f"   {split_name}: {len(split_samples)} samples")
            
            return True
        else:
            print("‚ö†Ô∏è  No processed data found - run data processing first")
            return False
            
    except Exception as e:
        print(f"‚ùå Data processing test failed: {e}")
        return False

def test_tokenization():
    """Test tokenization functionality."""
    print("\n" + "="*60)
    print("üî§ Testing Tokenization")
    print("="*60)
    
    try:
        from src.tokenization import ViT5Tokenizer
        
        # Initialize tokenizer
        tokenizer = ViT5Tokenizer(max_length=128)  # Smaller for testing
        print("‚úÖ ViT5Tokenizer initialized")
        print(f"   Model: VietAI/vit5-base")
        print(f"   Vocab size: {tokenizer.get_vocab_size()}")
        print(f"   Max length: {tokenizer.max_length}")
        
        # Test single tokenization
        sample_text = "T√¥i y√™u Vi·ªát Nam"
        sample_amr = "(y / y√™u :ARG0 (t / t√¥i) :ARG1 (v / Vi·ªát_Nam))"
        
        result = tokenizer.tokenize_sample(sample_text, sample_amr)
        print(f"‚úÖ Single tokenization:")
        print(f"   Input: '{sample_text}'")
        print(f"   Input tokens: {len(result.input_ids)}")
        print(f"   Label tokens: {len(result.labels)}")
        
        # Test batch tokenization
        batch_inputs = [
            "T√¥i y√™u Vi·ªát Nam",
            "C√¥ ·∫•y ƒëang h·ªçc ti·∫øng Anh",
            "H√¥m nay tr·ªùi ƒë·∫πp"
        ]
        batch_outputs = [
            "(y / y√™u :ARG0 (t / t√¥i) :ARG1 (v / Vi·ªát_Nam))",
            "(h / h·ªçc :ARG0 (c / c√¥_·∫•y) :ARG1 (t / ti·∫øng_Anh))",
            "(ƒë / ƒë·∫πp :ARG1 (t / tr·ªùi) :time (h / h√¥m_nay))"
        ]
        
        batch_results = tokenizer.tokenize_batch(batch_inputs, batch_outputs)
        print(f"‚úÖ Batch tokenization:")
        print(f"   Batch size: {len(batch_results)}")
        
        # Test decoding
        decoded = tokenizer.decode_tokens(result.input_ids[:10])
        print(f"‚úÖ Token decoding works")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Tokenization test failed: {e}")
        return False

def test_configuration():
    """Test configuration management."""
    print("\n" + "="*60)
    print("‚öôÔ∏è  Testing Configuration")
    print("="*60)
    
    try:
        from src.utils.config import Config
        
        # Test loading default config
        config = Config()
        print("‚úÖ Default configuration loaded")
        
        # Test loading from file
        if Path("config/local_test_config.yaml").exists():
            config = Config(config_file="config/local_test_config.yaml")
            print("‚úÖ YAML configuration loaded")
            
            print(f"   Model: {config.get('model', 'name', 'N/A')}")
            print(f"   Batch size: {config.get('model', 'batch_size', 'N/A')}")
            print(f"   Max samples: {config.get('data', 'max_samples', 'N/A')}")
            
            # Test validation
            config.validate()
            print("‚úÖ Configuration validation passed")
            
        return True
        
    except Exception as e:
        print(f"‚ùå Configuration test failed: {e}")
        return False

def test_evaluation_components():
    """Test evaluation components (without trained model)."""
    print("\n" + "="*60)
    print("üìä Testing Evaluation Components")
    print("="*60)
    
    try:
        from src.evaluation.evaluator import EvaluationMetrics
        
        # Test metrics data class
        metrics = EvaluationMetrics(
            bleu_1=0.5, bleu_2=0.4, bleu_3=0.3, bleu_4=0.2,
            rouge_1=0.6, rouge_2=0.5, rouge_l=0.55,
            exact_match=0.1
        )
        
        print("‚úÖ EvaluationMetrics created")
        print(f"   BLEU-4: {metrics.bleu_4}")
        print(f"   ROUGE-L: {metrics.rouge_l}")
        print(f"   Exact Match: {metrics.exact_match}")
        
        # Test metrics conversion
        metrics_dict = metrics.to_dict()
        print(f"‚úÖ Metrics conversion: {len(metrics_dict)} metrics")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Evaluation test failed: {e}")
        return False

def test_inference_components():
    """Test inference components (without trained model)."""
    print("\n" + "="*60)
    print("üîÆ Testing Inference Components")
    print("="*60)
    
    try:
        # Test AMR formatting
        raw_amr = "(y / y√™u :ARG0 (t / t√¥i) :ARG1 (v / Vi·ªát_Nam))"
        
        # Simple formatting test
        formatted = raw_amr.replace(" :", "\n   :")
        print("‚úÖ AMR formatting logic works")
        print(f"   Raw: {raw_amr}")
        print(f"   Formatted preview: {formatted[:50]}...")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Inference test failed: {e}")
        return False

def test_cli_interface():
    """Test CLI interface components."""
    print("\n" + "="*60)
    print("üíª Testing CLI Interface")
    print("="*60)
    
    try:
        # Test main.py imports
        import main
        print("‚úÖ main.py imports successfully")
        
        # Test argument parser
        parser = main.setup_argument_parser()
        print("‚úÖ Argument parser created")
        
        # Test help
        help_text = parser.format_help()
        print(f"‚úÖ Help text generated ({len(help_text)} characters)")
        
        return True
        
    except Exception as e:
        print(f"‚ùå CLI interface test failed: {e}")
        return False

def run_comprehensive_test():
    """Run all tests and provide summary."""
    print("üöÄ AMR Semantic Parsing - Comprehensive Test Suite")
    print("=" * 80)
    
    tests = [
        ("Data Processing", test_data_processing),
        ("Tokenization", test_tokenization),
        ("Configuration", test_configuration),
        ("Evaluation Components", test_evaluation_components),
        ("Inference Components", test_inference_components),
        ("CLI Interface", test_cli_interface)
    ]
    
    results = {}
    passed = 0
    
    for test_name, test_func in tests:
        try:
            result = test_func()
            results[test_name] = result
            if result:
                passed += 1
        except Exception as e:
            print(f"‚ùå {test_name} crashed: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "="*80)
    print("üìä TEST SUMMARY")
    print("="*80)
    
    for test_name, result in results.items():
        status = "‚úÖ PASS" if result else "‚ùå FAIL"
        print(f"{status:<10} {test_name}")
    
    print(f"\nüéØ Overall: {passed}/{len(tests)} tests passed")
    
    if passed == len(tests):
        print("\nüéâ ALL TESTS PASSED! System is ready for use.")
        print("\nüìã What works:")
        print("‚úÖ Data processing (1893 samples)")
        print("‚úÖ VietAI/vit5-base tokenization")
        print("‚úÖ Configuration management")
        print("‚úÖ Evaluation metrics")
        print("‚úÖ CLI interface")
        
        print("\n‚ö†Ô∏è  Training requires fixing type conversion issues")
        print("üí° For now, you can use the system for:")
        print("   - Data processing")
        print("   - Tokenization testing")
        print("   - Configuration management")
        print("   - Colab training (with notebook)")
        
    else:
        print(f"\n‚ö†Ô∏è  {len(tests) - passed} tests failed. Check errors above.")
    
    return passed == len(tests)

if __name__ == "__main__":
    run_comprehensive_test()
