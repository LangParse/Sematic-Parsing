# üöÄ Deployment Guide - Vietnamese AMR Semantic Parsing

Complete guide for deploying your Vietnamese AMR model to Hugging Face and creating web interfaces.

## üìã Prerequisites

1. **Trained Model**: You need a trained AMR model (e.g., `models/minimal_model/`)
2. **Hugging Face Account**: Create account at [huggingface.co](https://huggingface.co)
3. **HF Token**: Get your token from [Settings > Access Tokens](https://huggingface.co/settings/tokens)

## üîß Installation

Install deployment dependencies:

```bash
# Activate environment
source .venv/bin/activate

# Install deployment packages
pip install gradio huggingface_hub

# Or use the deployment script
python deploy.py install
```

## ü§ó Push Model to Hugging Face

### Method 1: Using CLI

```bash
# Set your HF token (optional - can use --token instead)
export HF_TOKEN="your_huggingface_token_here"

# Push model to HF Hub
python main.py push-model \
  --model-path models/minimal_model \
  --repo-name "your-username/vietnamese-amr-model" \
  --private  # Optional: make repository private
```

### Method 2: Using Deploy Script

```bash
# Push model with deploy script
python deploy.py push \
  --model-path models/minimal_model \
  --repo-name "your-username/vietnamese-amr-model" \
  --token "your_token_here" \
  --private
```

### What Gets Uploaded:

- ‚úÖ **Model files** (pytorch_model.bin, config.json)
- ‚úÖ **Tokenizer files** (tokenizer.json, special_tokens_map.json)
- ‚úÖ **Model card** (README.md with usage instructions)
- ‚úÖ **Metadata** (model tags, language info)

## üåê Gradio Web Interface

### Local Gradio App

#### Option 1: Mock Model (for testing)
```bash
# Run with mock model (no real model needed)
python gradio_app.py --port 7860

# Or via CLI
python main.py gradio --port 7860
```

#### Option 2: Local Model
```bash
# Run with your trained local model
python gradio_app.py --model-path models/minimal_model --port 7860

# Or via CLI
python main.py gradio --model-path models/minimal_model --port 7860
```

#### Option 3: Hugging Face Model
```bash
# Run with model from HF Hub
python gradio_app.py --hf-model "your-username/vietnamese-amr-model" --port 7860

# Or via CLI
python main.py gradio --hf-model "your-username/vietnamese-amr-model" --port 7860
```

#### Create Public Link
```bash
# Create public shareable link
python gradio_app.py --share --port 7860
```

### Gradio Interface Features

- üáªüá≥ **Vietnamese Interface**: Optimized for Vietnamese text
- üìù **Text Input**: Large text area for Vietnamese sentences
- üéØ **AMR Output**: Formatted AMR representation
- üîÑ **Clear/Submit**: Easy-to-use buttons
- üìö **Examples**: Pre-loaded Vietnamese examples
- üé® **Custom Styling**: Clean, professional design

## üèóÔ∏è Hugging Face Spaces Deployment

### Step 1: Create Space Files

```bash
# Generate HF Space files
python deploy.py space --repo-name "your-username/vietnamese-amr-model"
```

This creates:
- `app.py` - Main Gradio app for HF Spaces
- `requirements_space.txt` - Dependencies for Spaces
- `README_space.md` - Space description

### Step 2: Create Hugging Face Space

1. Go to [huggingface.co/new-space](https://huggingface.co/new-space)
2. Choose **Gradio** as SDK
3. Name your space (e.g., `vietnamese-amr-parser`)
4. Upload the generated files:
   - `app.py` ‚Üí root directory
   - `requirements_space.txt` ‚Üí rename to `requirements.txt`
   - `README_space.md` ‚Üí rename to `README.md`
   - `gradio_app.py` ‚Üí upload as is

### Step 3: Configure Space

Your Space will be available at:
`https://huggingface.co/spaces/your-username/your-space-name`

## üöÄ Full Deployment Workflow

### Complete deployment in one command:

```bash
python deploy.py full \
  --model-path models/minimal_model \
  --repo-name "your-username/vietnamese-amr-model" \
  --token "your_token_here" \
  --port 7860 \
  --share
```

This will:
1. ‚úÖ Install requirements
2. ‚úÖ Push model to HF Hub
3. ‚úÖ Create Space files
4. ‚úÖ Launch local Gradio app

## üìä Usage Examples

### 1. Test with Vietnamese Sentences

```
Input: "T√¥i y√™u Vi·ªát Nam"
Output: (y / y√™u 
   :ARG0 (t / t√¥i) 
   :ARG1 (v / Vi·ªát_Nam))

Input: "C√¥ ·∫•y ƒëang h·ªçc ti·∫øng Anh"
Output: (h / h·ªçc 
   :ARG0 (c / c√¥_·∫•y) 
   :ARG1 (t / ti·∫øng_Anh) 
   :aspect (p / progressive))
```

### 2. API Usage (after HF deployment)

```python
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Load your deployed model
tokenizer = AutoTokenizer.from_pretrained("your-username/vietnamese-amr-model")
model = AutoModelForSeq2SeqLM.from_pretrained("your-username/vietnamese-amr-model")

# Predict AMR
text = "T√¥i y√™u Vi·ªát Nam"
inputs = tokenizer(text, return_tensors="pt", max_length=512, truncation=True)
outputs = model.generate(**inputs, max_length=512, num_beams=4)
amr = tokenizer.decode(outputs[0], skip_special_tokens=True)
print(amr)
```

## üîß Configuration Options

### Gradio App Options

```bash
python gradio_app.py \
  --model-path models/your_model \     # Local model path
  --hf-model "username/model-name" \   # HF model name
  --port 7860 \                        # Port number
  --host 0.0.0.0 \                     # Host (for server deployment)
  --share \                            # Create public link
  --debug                              # Enable debug mode
```

### Push Model Options

```bash
python main.py push-model \
  --model-path models/your_model \     # Local model path
  --repo-name "username/model-name" \  # HF repository name
  --token "your_token" \               # HF token
  --private \                          # Make repository private
  --commit-message "Upload AMR model"  # Custom commit message
```

## üéØ Production Deployment

### For Production Servers:

```bash
# Run on all interfaces
python gradio_app.py --host 0.0.0.0 --port 8080

# With environment variables
export HF_MODEL="your-username/vietnamese-amr-model"
export PORT=8080
python gradio_app.py --hf-model $HF_MODEL --port $PORT --host 0.0.0.0
```

### Docker Deployment:

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY . .

RUN pip install -r requirements.txt
EXPOSE 7860

CMD ["python", "gradio_app.py", "--hf-model", "your-username/vietnamese-amr-model", "--host", "0.0.0.0", "--port", "7860"]
```

## üîç Troubleshooting

### Common Issues:

1. **Model not found**: Ensure model path exists or HF model name is correct
2. **Token issues**: Check HF token permissions and validity
3. **Memory issues**: Use smaller batch sizes or reduce model size
4. **Port conflicts**: Change port number if 7860 is occupied

### Debug Mode:

```bash
# Enable debug logging
python gradio_app.py --debug

# Check model loading
python -c "
from gradio_app import AMRGradioApp
app = AMRGradioApp(hf_model_name='your-username/vietnamese-amr-model')
print('Model loaded successfully!')
"
```

## üéâ Success!

After deployment, you'll have:

- ‚úÖ **HF Model Repository**: `https://huggingface.co/your-username/vietnamese-amr-model`
- ‚úÖ **Local Gradio App**: `http://localhost:7860`
- ‚úÖ **Public Gradio Link**: `https://xxxxx.gradio.live` (if using --share)
- ‚úÖ **HF Space**: `https://huggingface.co/spaces/your-username/your-space-name`

Your Vietnamese AMR model is now accessible worldwide! üåç
