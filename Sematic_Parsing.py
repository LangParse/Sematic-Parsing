# TrÆ°á»›c khi cháº¡y code lÆ°u Ã½:
# Thay tháº¿ Ä‘Æ°á»ng dáº«n "/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/" báº±ng Ä‘Æ°á»ng dáº«n hiá»‡n táº¡i trÃªn GG Drive cá»§a tá»«ng ngÆ°á»i Ä‘á»ƒ khÃ´ng bá»‹ bÃ¡o lá»—i File not Found
# VÃ­ dá»¥: ÄÆ°á»ng dáº«n trÃªn GG Drive cá»§a thÆ° má»¥c chá»©a toÃ n bá»™ Project vÃ  cÃ¡c file dá»¯ liá»‡u lÃ  "/content/drive/OtherDriveName/Project/AMR/"
# Replace "/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/" báº±ng "/content/drive/OtherDriveName/Project/AMR/"
# File training ban Ä‘áº§u cá»§a Ban Tá»• Chá»©c lÃ  2 file text. Anh Ä‘Ã£ gá»™p láº¡i thÃ nh 1 file vÃ  Ä‘áº·t tÃªn thÃ nh train_amr.txt. Bá» file nÃ y vÃ o Ä‘Ãºng Ä‘Æ°á»ng dáº«n /content/drive/OtherDriveName/Project/AMR/ má»›i cháº¡y Ä‘Æ°á»£c Module 1
# Thá»© tá»± táº¡o cÃ¡c file nhÆ° sau:
# Module 1: Input file train_amr.txt Ä‘á»ƒ lÃ m sáº¡ch dá»¯ liá»‡u vÃ  lÆ°u thÃ nh file input_amr.txt
# Module 2: Input file input_amr.txt Ä‘á»ƒ táº¡o thÃ nh file JSON (amr_data.json)
# Module 3: Input file amr_data,json Ä‘á»ƒ Tokenize dá»¯ liá»‡u phá»¥c vá»¥ cho mÃ´ hÃ¬nh training
# Module 4: Tá»« dá»¯ liá»‡u Ä‘Ã£ Tokenize, tiáº¿n trÃ¬nh training báº¯t Ä‘áº§u. NhÃºng chá»©c nÄƒng wandb Ä‘á»ƒ theo dÃµi tiáº¿n trÃ¬nh há»c. Cáº§n Ä‘Äƒng kÃ½ tÃ i khoáº£n wandb Ä‘á»ƒ láº¥y API (search GG)
# CÃ¡c module cÃ²n láº¡i khÃ´ng táº¡o ra file.

# =======================================================
# ğŸ“˜ MODULE 1: LÃ m sáº¡ch vÃ  chuáº©n hÃ³a dá»¯ liá»‡u AMR Ä‘áº§u vÃ o
# =======================================================

# =========================
# ğŸ“¦ IMPORT THÆ¯ VIá»†N
# =========================
import os
# ğŸ“Œ Ná»™i dung: ThÆ° viá»‡n há»— trá»£ thao tÃ¡c vá»›i Ä‘Æ°á»ng dáº«n file vÃ  thÆ° má»¥c
# ğŸ¯ Má»¥c Ä‘Ã­ch: Äáº£m báº£o viá»‡c Ä‘á»c/ghi file á»Ÿ Ä‘Ãºng vá»‹ trÃ­ vá»›i tÃªn chuáº©n
# âœ… Káº¿t quáº£: CÃ³ thá»ƒ trÃ­ch xuáº¥t thÆ° má»¥c cá»§a file gá»‘c vÃ  lÆ°u output vÃ o cÃ¹ng thÆ° má»¥c


# =========================
# ğŸ§¼ HÃ€M Xá»¬ LÃ FILE AMR
# =========================
def clean_amr_file(input_file_path):
    """
    ğŸ“Œ Ná»™i dung: Chuáº©n hÃ³a file AMR Ä‘áº§u vÃ o Ä‘á»ƒ dá»… xá»­ lÃ½ vá» sau.
    ğŸ¯ Má»¥c Ä‘Ã­ch: Gom nhÃ³m má»—i cÃ¢u tiáº¿ng Viá»‡t vá»›i sÆ¡ Ä‘á»“ AMR tÆ°Æ¡ng á»©ng thÃ nh 1 block, phÃ¢n cÃ¡ch báº±ng dÃ²ng '---'.
    âœ… Káº¿t quáº£: Táº¡o file 'input_amr.txt' á»Ÿ cÃ¹ng thÆ° má»¥c, Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ token hÃ³a (Module 2).
    """

    # ------------------------------------
    # BÆ°á»›c 1: Láº¥y thÆ° má»¥c cá»§a file Ä‘áº§u vÃ o
    # ------------------------------------
    dir_path = os.path.dirname(input_file_path)
    # ğŸ“Œ Ná»™i dung: TÃ¡ch pháº§n thÆ° má»¥c tá»« Ä‘Æ°á»ng dáº«n gá»‘c
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Äá»ƒ sau Ä‘Ã³ cÃ³ thá»ƒ táº¡o file output cÃ¹ng thÆ° má»¥c
    # âœ… Káº¿t quáº£: Biáº¿n `dir_path` chá»©a Ä‘Æ°á»ng dáº«n thÆ° má»¥c file gá»‘c

    # -------------------------------------
    # BÆ°á»›c 2: XÃ¡c Ä‘á»‹nh Ä‘Æ°á»ng dáº«n file output
    # -------------------------------------
    output_file_path = os.path.join(dir_path, "input_amr.txt")
    # ğŸ“Œ Ná»™i dung: Gá»™p Ä‘Æ°á»ng dáº«n thÆ° má»¥c vá»›i tÃªn file má»›i
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Äáº·t tÃªn thá»‘ng nháº¥t cho file AMR Ä‘Ã£ chuáº©n hÃ³a
    # âœ… Káº¿t quáº£: File output sáº½ náº±m á»Ÿ cÃ¹ng thÆ° má»¥c vá»›i tÃªn rÃµ rÃ ng

    # ------------------------------------------
    # BÆ°á»›c 3: Äá»c toÃ n bá»™ ná»™i dung tá»« file gá»‘c
    # ------------------------------------------
    with open(input_file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # ğŸ“Œ Ná»™i dung: Äá»c tá»«ng dÃ²ng tá»« file
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Chuáº©n bá»‹ dá»¯ liá»‡u Ä‘á»ƒ gom theo tá»«ng Ä‘oáº¡n AMR
    # âœ… Káº¿t quáº£: Danh sÃ¡ch `lines` chá»©a toÃ n bá»™ ná»™i dung cá»§a file

    # ------------------------------------------
    # BÆ°á»›c 4: Gom nhÃ³m cÃ¡c Ä‘oáº¡n AMR theo tá»«ng cÃ¢u
    # ------------------------------------------
    blocks = []          # ğŸ“Œ Danh sÃ¡ch chá»©a cÃ¡c block gá»“m cÃ¢u vÃ  AMR
    current_block = []   # ğŸ“Œ Danh sÃ¡ch táº¡m Ä‘á»ƒ gom tá»«ng block

    for line in lines:
        line = line.rstrip()  # ğŸ“Œ XoÃ¡ kÃ½ tá»± xuá»‘ng dÃ²ng
                              # ğŸ¯ Má»¥c Ä‘Ã­ch: TrÃ¡nh bá»‹ xuá»‘ng dÃ²ng thá»«a khi ghi láº¡i file

        if line.startswith("#::snt"):  # ğŸ“Œ DÃ²ng báº¯t Ä‘áº§u lÃ  cÃ¢u tiáº¿ng Viá»‡t
            if current_block:
                blocks.append(current_block)
                # ğŸ¯ Má»¥c Ä‘Ã­ch: Náº¿u Ä‘ang xá»­ lÃ½ má»™t block cÅ©, thÃ¬ thÃªm vÃ o danh sÃ¡ch chÃ­nh
                # âœ… Káº¿t quáº£: Má»—i block Ä‘Æ°á»£c phÃ¢n biá»‡t rÃµ rÃ ng

                current_block = []  # ğŸ“Œ Reset block Ä‘á»ƒ báº¯t Ä‘áº§u Ä‘oáº¡n má»›i

        current_block.append(line)  # ğŸ“Œ ThÃªm dÃ²ng hiá»‡n táº¡i vÃ o block hiá»‡n táº¡i

    # ------------------------------------------
    # BÆ°á»›c 5: ThÃªm Ä‘oáº¡n cuá»‘i náº¿u chÆ°a Ä‘Æ°á»£c ghi
    # ------------------------------------------
    if current_block:
        blocks.append(current_block)
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Äáº£m báº£o Ä‘oáº¡n cuá»‘i cÃ¹ng khÃ´ng bá»‹ bá» sÃ³t
    # âœ… Káº¿t quáº£: Danh sÃ¡ch `blocks` Ä‘áº§y Ä‘á»§ táº¥t cáº£ Ä‘oáº¡n cÃ¢u + AMR

    # ------------------------------------------
    # BÆ°á»›c 6: Ghi cÃ¡c block vÃ o file má»›i
    # ------------------------------------------
    with open(output_file_path, 'w', encoding='utf-8') as f:
        for block in blocks:
            for line in block:
                f.write(line + '\n')
            f.write('-----------------------------------------------\n')
    # ğŸ“Œ Ná»™i dung: Ghi tá»«ng dÃ²ng trong block vÃ  phÃ¢n cÃ¡ch cÃ¡c block báº±ng dÃ²ng gáº¡ch ngang
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Dá»… dÃ ng phÃ¢n tÃ­ch tá»«ng Ä‘oáº¡n trong cÃ¡c bÆ°á»›c tiáº¿p theo
    # âœ… Káº¿t quáº£: File `input_amr.txt` Ä‘Ã£ sáºµn sÃ ng Ä‘á»ƒ dÃ¹ng tiáº¿p trong pipeline

    # ------------------------------------------
    # BÆ°á»›c 7: In ra Ä‘Æ°á»ng dáº«n file káº¿t quáº£
    # ------------------------------------------
    print(f"âœ… File AMR Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a vÃ  lÆ°u táº¡i: {output_file_path}")
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Cho ngÆ°á»i dÃ¹ng xÃ¡c nháº­n quÃ¡ trÃ¬nh xá»­ lÃ½ Ä‘Ã£ xong
    # âœ… Káº¿t quáº£: Biáº¿t chÃ­nh xÃ¡c nÆ¡i lÆ°u file Ä‘á»ƒ tiáº¿p tá»¥c Module 2


# =========================
# â–¶ï¸ Gá»ŒI HÃ€M TIá»€N Xá»¬ LÃ
# =========================
clean_amr_file("/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/train_amr.txt")
# ğŸ“Œ Ná»™i dung: Cháº¡y hÃ m vá»›i Ä‘Æ°á»ng dáº«n cá»¥ thá»ƒ tá»›i file AMR gá»‘c
# ğŸ¯ Má»¥c Ä‘Ã­ch: Chuáº©n hÃ³a ná»™i dung Ä‘á»ƒ phá»¥c vá»¥ Module 2
# âœ… Káº¿t quáº£: Táº¡o Ä‘Æ°á»£c file `input_amr.txt` á»Ÿ thÆ° má»¥c gá»‘c, cáº¥u trÃºc rÃµ rÃ ng


# =======================================
# ğŸ“¦ MODULE 2: CHUYá»‚N AMR THÃ€NH JSON
# =======================================

import json  # âœª Ná»™i dung: DÃ¹ng Ä‘á»ƒ Ä‘á»c/ghi file JSON
              # ğŸŒŸ Má»¥c Ä‘Ã­ch: Biáº¿n danh sÃ¡ch cáº·p (input/output) thÃ nh file training
              # âœ… Káº¿t quáº£: Cho ra file .json chuáº©n huáº¥n luyá»‡n

import os    # âœª Ná»™i dung: DÃ¹ng Ä‘á»ƒ xá»­ lÃ½ Ä‘Æ°á»ng dáº«n file
              # ğŸŒŸ Má»¥c Ä‘Ã­ch: Gá»«i gá»n viá»‡c chá»‰nh sá»­a path file
              # âœ… Káº¿t quáº£: Biáº¿n file Ä‘á»“ng bá»™ vá»›i Module 1

# =======================================
# âœ… KHAI BÃO ÄÆ¯á»šNG DáºªN INPUT/OUTPUT
# =======================================
input_file = "/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/input_amr.txt"  # âœª File AMR sau khi clean
output_file = "/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_data.json" # âœª File json huáº¥n luyá»‡n

# =======================================
# âœ… BÆ¯á»šC 1: Äá»ŒC FILE VÃ€ LÆ¯U Dá»¯ LIá»†U Tá»ªNG DÃ’NG
# =======================================
with open(input_file, 'r', encoding='utf-8') as f:
    lines = f.readlines()  # âœª Äá»c toÃ n bá»™ dá»¯ liá»‡u dÃ²ng
                        # ğŸŒŸ Cho phÃ©p duyá»‡t theo block
                        # âœ… Táº¡o danh sÃ¡ch lines Ä‘á»ƒ xá»­ lÃ½ dáº§n

# =======================================
# âœ… BÆ¯á»šC 2: TÃCH Cáº¶P INPUT/OUTPUT
# =======================================
data = []                 # âœª Danh sÃ¡ch dá»¯ liá»‡u output dáº¡ng JSON
current_sentence = None   # âœª LÆ°u cÃ¢u Ä‘ang xá»­ lÃ½
amr_lines = []            # âœª Gom cÃ¡c dÃ²ng AMR tÆ°Æ¡ng á»©ng

for line in lines:
    line = line.strip()  # âœª XÃ³a khoáº£ng tráº¯ng dÆ°

    if line.startswith("#::snt"):  # âœª Gáº·p cÃ¢u má»›i
        if current_sentence and amr_lines:
            data.append({  # âœª LÆ°u block trÆ°á»›c
                "input": current_sentence,
                "output": "\n".join(amr_lines)
            })
        current_sentence = line[7:].strip()  # âœª TÃ¡ch cÃ¢u
        amr_lines = []

    elif line == "-----------------------------------------------":
        if current_sentence and amr_lines:
            data.append({  # âœª LÆ°u block cuá»‘i cá»§a cÃ¢u
                "input": current_sentence,
                "output": "\n".join(amr_lines)
            })
            current_sentence = None
            amr_lines = []

    elif current_sentence is not None:
        amr_lines.append(line)  # âœª Gom dÃ²ng AMR

# âœª Náº¿u cÃ¢u cuá»‘i chÆ°a Ä‘Æ°á»£c lÆ°u
if current_sentence and amr_lines:
    data.append({
        "input": current_sentence,
        "output": "\n".join(amr_lines)
    })

# =======================================
# âœ… BÆ¯á»šC 3: LOáº I Bá»† TRÃ™NG Láº¤P
# =======================================
seen = set()             # âœª Set Ä‘á»ƒ kiá»ƒm tra cÃ¡c cáº·p trÃ¹ng
filtered_data = []       # âœª Danh sÃ¡ch Ä‘Ã£ loáº¡i trÃ¹ng
for entry in data:
    key = (entry["input"], entry["output"])
    if key not in seen:
        filtered_data.append(entry)
        seen.add(key)  # âœª ÄÃ¡nh dáº¥u Ä‘Ã£ gáº·p

# =======================================
# âœ… BÆ¯á»šC 4: GHI RA FILE JSON
# =======================================
with open(output_file, 'w', encoding='utf-8') as f:
    json.dump(filtered_data, f, ensure_ascii=False, indent=2)  # âœª Ghi chuáº©n UTF-8, dá»… Ä‘á»c

# =======================================
# âœ… BÆ¯á»šC 5: THÃ”NG BÃO HOÃ€N THÃ€NH
# =======================================
print(f"âœ… ÄÃ£ lÆ°u {len(filtered_data)} cáº·p input/output vÃ o: {output_file}")


# ==============================================================================
# ğŸ“¦ MODULE 3: TOKENIZE Dá»® LIá»†U Tá»ª FILE JSON Äá»‚ CHUáº¨N Bá»Š CHO MÃ” HÃŒNH HUáº¤N LUYá»†N
# ==============================================================================

# =========================
# ğŸ“¦ IMPORT CÃC THÆ¯ VIá»†N
# =========================
from transformers import T5Tokenizer        # DÃ¹ng tokenizer cá»§a mÃ´ hÃ¬nh T5 hoáº·c LongT5
import json                                # Äá»c/ghi file JSON
import os                                  # Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n file

# =========================
# ğŸ›  Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# =========================
input_json_path = "/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_data.json"
token_output_path = os.path.join(os.path.dirname(input_json_path), "Tokenize_data.txt")

# =========================
# âœ‚ï¸ Táº¢I TOKENIZER
# =========================
tokenizer = T5Tokenizer.from_pretrained("google/long-t5-tglobal-base")  # DÃ¹ng tokenizer tÆ°Æ¡ng thÃ­ch vá»›i LongT5

# =========================
# ğŸ”„ TOKENIZE TOÃ€N Bá»˜ Dá»® LIá»†U
# =========================
with open(input_json_path, 'r', encoding='utf-8') as f:
    data = json.load(f)  # Äá»c toÃ n bá»™ dá»¯ liá»‡u {"input": ..., "output": ...}

with open(token_output_path, 'w', encoding='utf-8') as f_out:
    for item in data:
        input_text = item["input"]
        output_text = item["output"]

        # Tokenize input
        input_tokens = tokenizer.tokenize(input_text)
        # Tokenize output
        output_tokens = tokenizer.tokenize(output_text)

        # Ghi thÃ´ng tin ra file
        f_out.write("#::input_tokens\n")
        f_out.write(" ".join(input_tokens) + "\n\n")

        f_out.write("#::output_tokens\n")
        f_out.write(" ".join(output_tokens) + "\n")

        # PhÃ¢n cÃ¡ch giá»¯a cÃ¡c cÃ¢u
        f_out.write("-" * 40 + "\n")
		
		
# =========================================================================================
# ğŸ“¦ MODULE 4: HUáº¤N LUYá»†N MÃ” HÃŒNH vit5-base Äá»‚ CHUYá»‚N Äá»”I Tá»ª CÃ‚U TIáº¾NG VIá»†T SANG SÆ  Äá»’ AMR
# =========================================================================================

# =========================
# ğŸ“¦ IMPORT CÃC THÆ¯ VIá»†N
# =========================
from transformers import T5ForConditionalGeneration                 # ğŸ“Œ MÃ´ hÃ¬nh seq2seq T5
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments   # ğŸ“Œ Trainer cho mÃ´ hÃ¬nh Seq2Seq
from datasets import Dataset                                        # ğŸ“Œ DÃ¹ng Ä‘á»ƒ táº¡o dataset tá»« JSON list
from transformers import AutoTokenizer                              # ğŸ“Œ Tokenizer phÃ¹ há»£p vá»›i mÃ´ hÃ¬nh
import json                                                         # ğŸ“Œ DÃ¹ng Ä‘á»c file dá»¯ liá»‡u
import os                                                           # ğŸ“Œ Quáº£n lÃ½ file path
import wandb                                                        # ğŸ“Œ Theo dÃµi training trÃªn wandb

# =========================
# ğŸ›  Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN
# =========================
input_json_path = "/content/drive/MyDrive/Colab Notebooks/Sematic Parsing/amr_data.json"
model_output_dir = "/content/drive/MyDrive/Colab Notebooks/Sematic Parsing/amr_model_vit5"
log_dir = "/content/drive/MyDrive/Colab Notebooks/Sematic Parsing/logs_vit5"

# =========================
# ğŸ§¾ Káº¾T Ná»I WANDB
# =========================
wandb.login()  
# ğŸ“Œ Ná»™i dung: ÄÄƒng nháº­p tÃ i khoáº£n wandb Ä‘Ã£ cáº¥u hÃ¬nh tá»« trÆ°á»›c
# ğŸ¯ Má»¥c Ä‘Ã­ch: Báº¯t Ä‘áº§u theo dÃµi tiáº¿n trÃ¬nh training
# âœ… Káº¿t quáº£: Log mÃ´ hÃ¬nh, biá»ƒu Ä‘á»“ loss hiá»ƒn thá»‹ trÃªn trang wandb.io

# =========================
# âœ‚ï¸ LOAD TOKENIZER VIT5
# =========================
tokenizer = AutoTokenizer.from_pretrained("VietAI/vit5-base")  
# ğŸ“Œ Ná»™i dung: DÃ¹ng tokenizer tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh vit5
# ğŸ¯ Má»¥c Ä‘Ã­ch: Token hÃ³a input/output Ä‘Ãºng Ä‘á»‹nh dáº¡ng mÃ´ hÃ¬nh huáº¥n luyá»‡n
# âœ… Káº¿t quáº£: CÃ³ thá»ƒ sá»­ dá»¥ng .encode/.decode cho vÄƒn báº£n tiáº¿ng Viá»‡t

# =========================
# ğŸš€ HÃ€M HUáº¤N LUYá»†N
# =========================
def train_model(json_path):
    # BÆ°á»›c 1: Äá»c file JSON chá»©a cÃ¡c cáº·p (input/output)
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    # ğŸ“Œ Ná»™i dung: Load toÃ n bá»™ dá»¯ liá»‡u huáº¥n luyá»‡n
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Chuáº©n bá»‹ dá»¯ liá»‡u trÆ°á»›c khi Ä‘Æ°a vÃ o dataset
    # âœ… Káº¿t quáº£: Danh sÃ¡ch dict vá»›i cÃ¡c cáº·p input/output

    # BÆ°á»›c 2: Táº¡o Dataset tá»« danh sÃ¡ch dict
    dataset = Dataset.from_list(data)
    # ğŸ“Œ Ná»™i dung: Biáº¿n list â†’ dataset Ä‘á»ƒ Trainer sá»­ dá»¥ng
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Chuáº©n hÃ³a format theo HuggingFace
    # âœ… Káº¿t quáº£: Biáº¿n `dataset` cÃ³ thá»ƒ dÃ¹ng trá»±c tiáº¿p

    # BÆ°á»›c 3: Tokenize toÃ n bá»™ dá»¯ liá»‡u
    def preprocess(example):
        model_input = tokenizer(
            example["input"],
            padding="max_length",
            truncation=True,
            max_length=512
        )
        labels = tokenizer(
            example["output"],
            padding="max_length",
            truncation=True,
            max_length=512
        )
        model_input["labels"] = labels["input_ids"]
        return model_input

    tokenized_dataset = dataset.map(preprocess)
    # ğŸ“Œ Ná»™i dung: Tokenize toÃ n bá»™ cÃ¢u input/output
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Chuyá»ƒn vÄƒn báº£n thÃ nh ID Ä‘á»ƒ mÃ´ hÃ¬nh huáº¥n luyá»‡n
    # âœ… Káº¿t quáº£: Dataset Ä‘Ã£ token hÃ³a Ä‘áº§y Ä‘á»§

    # BÆ°á»›c 4: Load mÃ´ hÃ¬nh vit5-base
    model = T5ForConditionalGeneration.from_pretrained("VietAI/vit5-base")
    # ğŸ“Œ Ná»™i dung: Sá»­ dá»¥ng mÃ´ hÃ¬nh tiáº¿ng Viá»‡t Ä‘Ã£ pretrain
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Dá»±a trÃªn kiáº¿n thá»©c cÅ© Ä‘á»ƒ fine-tune nhanh hÆ¡n
    # âœ… Káº¿t quáº£: MÃ´ hÃ¬nh Ä‘Ã£ sáºµn sÃ ng huáº¥n luyá»‡n

    # BÆ°á»›c 5: Cáº¥u hÃ¬nh huáº¥n luyá»‡n
    training_args = Seq2SeqTrainingArguments(
        output_dir=model_output_dir,              
        per_device_train_batch_size=4,             # T4 nÃªn dÃ¹ng batch 4
        num_train_epochs=10,                       # CÃ³ thá»ƒ Ä‘iá»u chá»‰nh náº¿u loss á»•n Ä‘á»‹nh
        logging_dir=log_dir,                       
        logging_steps=500,                         # Ghi log má»—i 500 steps
        save_steps=200,                            # LÆ°u model Ä‘á»‹nh ká»³
        save_total_limit=2,                        # Chá»‰ lÆ°u tá»‘i Ä‘a 2 model
        report_to=["wandb"],                       # Káº¿t ná»‘i wandb
        run_name="AMR_ViT5_T4GPU",                 # TÃªn run trong wandb
        fp16=True                                  # TÄƒng tá»‘c náº¿u GPU há»— trá»£
    )

    # BÆ°á»›c 6: Táº¡o Trainer vÃ  train
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        tokenizer=tokenizer
    )

    # BÆ°á»›c 7: Tiáº¿n hÃ nh huáº¥n luyá»‡n
    trainer.train()
    # ğŸ“Œ Ná»™i dung: Gá»i trainer Ä‘á»ƒ báº¯t Ä‘áº§u huáº¥n luyá»‡n
    # ğŸ¯ Má»¥c Ä‘Ã­ch: Fine-tune mÃ´ hÃ¬nh vit5 trÃªn dá»¯ liá»‡u AMR tiáº¿ng Viá»‡t
    # âœ… Káº¿t quáº£: Model Ä‘Æ°á»£c cáº£i thiá»‡n theo dá»¯ liá»‡u AMR

    return model

# =========================
# â–¶ï¸ Gá»ŒI HÃ€M HUáº¤N LUYá»†N
# =========================
model = train_model(input_json_path)
# ğŸ“Œ Ná»™i dung: Gá»i huáº¥n luyá»‡n tá»« file json
# ğŸ¯ Má»¥c Ä‘Ã­ch: KÃ­ch hoáº¡t toÃ n bá»™ pipeline training
# âœ… Káº¿t quáº£: MÃ´ hÃ¬nh vit5 Ä‘Æ°á»£c huáº¥n luyá»‡n vÃ  lÆ°u


# ====================================================================
# MODULE 5A: ÄÃNH GIÃ CHáº¤T LÆ¯á»¢NG MÃ” HÃŒNH T5 Báº°NG CÃCH TÃNH BLEU SCORE
# ====================================================================

# =========================
# ğŸ“¦ IMPORT Cáº¦N THIáº¾T
# =========================
from nltk.translate.bleu_score import sentence_bleu   # Ná»™i dung: HÃ m tÃ­nh Ä‘iá»ƒm BLEU tá»« NLTK
                                                      # Má»¥c Ä‘Ã­ch: So sÃ¡nh Ä‘á»™ giá»‘ng nhau giá»¯a AMR tháº­t vÃ  dá»± Ä‘oÃ¡n
                                                      # Káº¿t quáº£: Tráº£ vá» sá»‘ tá»« 0.0 Ä‘áº¿n 1.0

import json  # Ná»™i dung: DÃ¹ng Ä‘á»ƒ Ä‘á»c dá»¯ liá»‡u tá»« file JSON
             # Má»¥c Ä‘Ã­ch: Láº¥y cáº·p input/output Ä‘Ã£ lÆ°u
             # Káº¿t quáº£: Biáº¿n `samples` lÃ  list chá»©a nhiá»u dict

import torch  # Ná»™i dung: ThÆ° viá»‡n tÃ­nh toÃ¡n tensor
              # Má»¥c Ä‘Ã­ch: Kiá»ƒm tra vÃ  chuyá»ƒn device giá»¯a CPU vÃ  GPU
              # Káº¿t quáº£: Äáº£m báº£o model vÃ  dá»¯ liá»‡u náº±m trÃªn cÃ¹ng thiáº¿t bá»‹

# =========================
# ğŸ“ ÄÆ¯á»œNG DáºªN FILE
# =========================
json_path = "/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_data.json"  # ÄÆ°á»ng dáº«n file JSON Ä‘áº§u vÃ o

# =========================
# ğŸ§ª HÃ€M ÄÃNH GIÃ MÃ” HÃŒNH
# =========================
def evaluate_model(model, tokenizer, json_path):
    # BÆ°á»›c 1: Äá»c file chá»©a cáº·p input/output tá»« json
    with open(json_path, 'r', encoding='utf-8') as f:
        samples = json.load(f)

    total_score = 0  # BÆ°á»›c 2: Khá»Ÿi táº¡o biáº¿n tá»•ng Ä‘iá»ƒm BLEU

    # BÆ°á»›c 3: XÃ¡c Ä‘á»‹nh device mÃ´ hÃ¬nh Ä‘ang cháº¡y (GPU náº¿u cÃ³, khÃ´ng thÃ¬ CPU)
    device = next(model.parameters()).device

    # BÆ°á»›c 4: Láº·p 100 máº«u Ä‘áº§u Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ nhanh
    for sample in samples[:100]:
        # BÆ°á»›c 4.1: Token hÃ³a cÃ¢u Ä‘áº§u vÃ o vÃ  chuyá»ƒn sang Ä‘Ãºng device
        input_ids = tokenizer(sample["input"], return_tensors="pt").input_ids.to(device)

        # BÆ°á»›c 4.2: MÃ´ hÃ¬nh sinh sÆ¡ Ä‘á»“ AMR dá»± Ä‘oÃ¡n
        output_ids = model.generate(input_ids)[0]

        # BÆ°á»›c 4.3: Giáº£i mÃ£ chuá»—i dá»± Ä‘oÃ¡n thÃ nh vÄƒn báº£n
        prediction = tokenizer.decode(output_ids, skip_special_tokens=True)

        # BÆ°á»›c 4.4: TÃ¡ch chuá»—i thÃ nh danh sÃ¡ch tá»« (token) Ä‘á»ƒ tÃ­nh BLEU
        reference = sample["output"].split()      # AMR tháº­t
        hypothesis = prediction.split()           # AMR dá»± Ä‘oÃ¡n

        # BÆ°á»›c 4.5: Cá»™ng Ä‘iá»ƒm BLEU vÃ o tá»•ng
        total_score += sentence_bleu([reference], hypothesis)

    # BÆ°á»›c 5: TÃ­nh Ä‘iá»ƒm BLEU trung bÃ¬nh
    avg_bleu = round(total_score / 100, 4)

    # BÆ°á»›c 6: In káº¿t quáº£ cuá»‘i cÃ¹ng
    print("BLEU score:", avg_bleu)

# =========================
# â–¶ï¸ Gá»ŒI HÃ€M ÄÃNH GIÃ
# =========================
evaluate_model(model, tokenizer, json_path)


# ===========================================================================
# MODULE 6: LÆ¯U MÃ” HÃŒNH VÃ€ TOKENIZER ÄÃƒ HUáº¤N LUYá»†N VÃ€O THÆ¯ Má»¤C Äá»‚ Sá»¬ Dá»¤NG Láº I
# ===========================================================================

# Ná»™i dung: Gá»i phÆ°Æ¡ng thá»©c save_pretrained() cá»§a mÃ´ hÃ¬nh T5
# Má»¥c Ä‘Ã­ch: LÆ°u toÃ n bá»™ trá»ng sá»‘, kiáº¿n trÃºc vÃ  config cá»§a mÃ´ hÃ¬nh
# Káº¿t quáº£: Táº¡o thÆ° má»¥c /content/amr_model chá»©a cÃ¡c file nhÆ° config.json, pytorch_model.bin, ...
model.save_pretrained("/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_model")

# Ná»™i dung: LÆ°u tokenizer kÃ¨m theo mÃ´ hÃ¬nh
# Má»¥c Ä‘Ã­ch: Äáº£m báº£o khi load láº¡i mÃ´ hÃ¬nh váº«n dÃ¹ng Ä‘Ãºng kiá»ƒu token hÃ³a
# Káº¿t quáº£: ThÆ° má»¥c /content/amr_model cÃ³ thÃªm tokenizer_config.json, vocab.json, tokenizer.model, ...
tokenizer.save_pretrained("/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_model")


# ========================================================================================
# MODULE 7: CHO NGÆ¯á»œI DÃ™NG NHáº¬P Má»˜T CÃ‚U TIáº¾NG VIá»†T, MÃ” HÃŒNH Sáº¼ Dá»° ÄOÃN SÆ  Äá»’ AMR TÆ¯Æ NG á»¨NG
# ========================================================================================

from transformers import T5ForConditionalGeneration, T5Tokenizer  # Ná»™i dung: Import mÃ´ hÃ¬nh vÃ  tokenizer T5
                                                                  # Má»¥c Ä‘Ã­ch: DÃ¹ng Ä‘á»ƒ load láº¡i mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n vÃ  xá»­ lÃ½ cÃ¢u nháº­p
                                                                  # Káº¿t quáº£: CÃ³ thá»ƒ sá»­ dá»¥ng model.generate Ä‘á»ƒ táº¡o AMR tá»« input

def predict_amr_from_input():
    # Ná»™i dung: Load láº¡i mÃ´ hÃ¬nh Ä‘Ã£ lÆ°u trÆ°á»›c Ä‘Ã³
    # Má»¥c Ä‘Ã­ch: DÃ¹ng mÃ´ hÃ¬nh Ä‘Ã£ huáº¥n luyá»‡n thay vÃ¬ táº¡o má»›i
    # Káº¿t quáº£: Biáº¿n `model` chá»©a mÃ´ hÃ¬nh Ä‘Ã£ sáºµn sÃ ng dá»± Ä‘oÃ¡n
    model = T5ForConditionalGeneration.from_pretrained("/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_model")

    # Ná»™i dung: Load láº¡i tokenizer tÆ°Æ¡ng á»©ng vá»›i mÃ´ hÃ¬nh
    # Má»¥c Ä‘Ã­ch: Äáº£m báº£o mÃ´ hÃ¬nh vÃ  tokenizer Ä‘á»“ng bá»™
    # Káº¿t quáº£: Biáº¿n `tokenizer` sáºµn sÃ ng mÃ£ hÃ³a/giáº£i mÃ£ vÄƒn báº£n
    tokenizer = T5Tokenizer.from_pretrained("/content/drive/MyDrive/Colab Notebooks/Semantic Parsing/amr_model")

    # Ná»™i dung: Cho phÃ©p ngÆ°á»i dÃ¹ng nháº­p má»™t cÃ¢u tiáº¿ng Viá»‡t tá»« bÃ n phÃ­m
    # Má»¥c Ä‘Ã­ch: LÃ m input cho mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n
    # Káº¿t quáº£: CÃ¢u Ä‘Æ°á»£c lÆ°u vÃ o biáº¿n `input_sentence`, giá»¯ nguyÃªn dáº¥u tiáº¿ng Viá»‡t
    input_sentence = input("Nháº­p má»™t cÃ¢u tiáº¿ng Viá»‡t Ä‘á»ƒ chuyá»ƒn thÃ nh sÆ¡ Ä‘á»“ AMR: ").strip()

    # Ná»™i dung: Tokenize cÃ¢u nháº­p báº±ng tokenizer cá»§a mÃ´ hÃ¬nh
    # Má»¥c Ä‘Ã­ch: Chuyá»ƒn cÃ¢u thÃ nh tensor Ä‘á»ƒ mÃ´ hÃ¬nh xá»­ lÃ½
    # Káº¿t quáº£: Táº¡o input_ids dÃ¹ng cho mÃ´ hÃ¬nh
    input_ids = tokenizer(input_sentence, return_tensors="pt").input_ids

    # Ná»™i dung: Sinh Ä‘áº§u ra tá»« mÃ´ hÃ¬nh dá»±a trÃªn cÃ¢u Ä‘Ã£ token hÃ³a
    # Má»¥c Ä‘Ã­ch: Táº¡o sÆ¡ Ä‘á»“ AMR tá»« cÃ¢u tiáº¿ng Viá»‡t
    # Káº¿t quáº£: Biáº¿n `output_ids` chá»©a tensor biá»ƒu diá»…n AMR
    output_ids = model.generate(input_ids)[0]

    # Ná»™i dung: Giáº£i mÃ£ tensor Ä‘áº§u ra thÃ nh vÄƒn báº£n
    # Má»¥c Ä‘Ã­ch: Chuyá»ƒn tá»« token ID â†’ chuá»—i AMR
    # Káº¿t quáº£: Biáº¿n `prediction` lÃ  sÆ¡ Ä‘á»“ AMR dÆ°á»›i dáº¡ng chuá»—i
    prediction = tokenizer.decode(output_ids, skip_special_tokens=True)

    # Ná»™i dung: In káº¿t quáº£ sÆ¡ Ä‘á»“ AMR ra mÃ n hÃ¬nh
    # Má»¥c Ä‘Ã­ch: Hiá»ƒn thá»‹ cho ngÆ°á»i dÃ¹ng xem trá»±c tiáº¿p
    # Káº¿t quáº£: Káº¿t quáº£ Ä‘Æ°á»£c hiá»ƒn thá»‹ rÃµ rÃ ng
    print("\nğŸ“Œ SÆ¡ Ä‘á»“ AMR dá»± Ä‘oÃ¡n:\n")
    print(prediction)

# Ná»™i dung: Gá»i hÃ m Ä‘á»ƒ báº¯t Ä‘áº§u dá»± Ä‘oÃ¡n
# Má»¥c Ä‘Ã­ch: Cho phÃ©p nháº­p cÃ¢u vÃ  cháº¡y mÃ´ hÃ¬nh
# Káº¿t quáº£: MÃ´ hÃ¬nh in ra sÆ¡ Ä‘á»“ AMR tÆ°Æ¡ng á»©ng vá»›i cÃ¢u nháº­p
predict_amr_from_input()