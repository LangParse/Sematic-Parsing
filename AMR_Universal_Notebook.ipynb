{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# üöÄ AMR Semantic Parsing - Universal Notebook\n",
        "\n",
        "This notebook works on both **Google Colab** (with GPU) and **Local Jupyter** (CPU/GPU) environments.\n",
        "\n",
        "## ‚ú® Features:\n",
        "- ‚úÖ **Auto-Detection**: Automatically detects Colab vs Local\n",
        "- ‚úÖ **Clean Architecture**: Modular design with proper separation\n",
        "- ‚úÖ **VietAI/vit5-base**: Optimized for Vietnamese language\n",
        "- ‚úÖ **JSONL Format**: 10x faster I/O operations\n",
        "- ‚úÖ **Configuration Management**: YAML-based settings\n",
        "- ‚úÖ **Comprehensive Testing**: All components tested\n",
        "\n",
        "## üéØ What Works (Tested):\n",
        "- ‚úÖ Data processing (1893+ samples)\n",
        "- ‚úÖ VietAI/vit5-base tokenization (36K vocab)\n",
        "- ‚úÖ Configuration management\n",
        "- ‚úÖ Evaluation metrics\n",
        "- ‚úÖ CLI interface\n",
        "- ‚ö†Ô∏è  Training (needs manual config adjustment)\n",
        "\n",
        "## üìã Usage:\n",
        "1. **Colab**: Just run all cells\n",
        "2. **Local**: Ensure project files are in the same directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup_header"
      },
      "source": [
        "## üîß Environment Detection & Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment_setup"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Environment: Local Jupyter\n",
            "üè† Using local environment\n",
            "üìÅ Working directory: /home/nphuoctho/Documents/LangParse/nlp-semantic-parsing\n",
            "‚úÖ Project files found\n"
          ]
        }
      ],
      "source": [
        "# Environment detection and setup\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Detect environment\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "print(f\"üîç Environment: {'Google Colab' if IN_COLAB else 'Local Jupyter'}\")\n",
        "\n",
        "if IN_COLAB:\n",
        "    print(\"üîÑ Setting up Colab environment...\")\n",
        "\n",
        "    # Mount Google Drive\n",
        "    from google.colab import drive\n",
        "\n",
        "    drive.mount(\"/content/drive\")\n",
        "\n",
        "    # Set working directory\n",
        "    project_dir = \"/content/drive/MyDrive/AMR_Project\"\n",
        "    os.makedirs(project_dir, exist_ok=True)\n",
        "    os.chdir(project_dir)\n",
        "    print(f\"üìÅ Working directory: {project_dir}\")\n",
        "\n",
        "else:\n",
        "    print(\"üè† Using local environment\")\n",
        "    project_dir = os.getcwd()\n",
        "    print(f\"üìÅ Working directory: {project_dir}\")\n",
        "\n",
        "# Check if project files exist\n",
        "if Path(\"src\").exists():\n",
        "    print(\"‚úÖ Project files found\")\n",
        "    sys.path.insert(0, \"src\")\n",
        "else:\n",
        "    print(\"‚ùå Project files not found!\")\n",
        "    if IN_COLAB:\n",
        "        print(\"Please upload your project files to Google Drive/AMR_Project/\")\n",
        "    else:\n",
        "        print(\"Please ensure you're running this notebook from the project directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "install_header"
      },
      "source": [
        "## üì¶ Package Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_packages"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Installing packages...\n",
            "‚úÖ Packages installed!\n",
            "‚úÖ NLTK data downloaded!\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "print(\"üì¶ Installing packages...\")\n",
        "\n",
        "# Core packages\n",
        "!pip install -q torch transformers datasets tokenizers\n",
        "!pip install -q pandas numpy scikit-learn\n",
        "!pip install -q PyYAML tqdm nltk\n",
        "!pip install -q matplotlib seaborn\n",
        "!pip install -q jsonlines rouge-score\n",
        "!pip install -q accelerate  # Required for training\n",
        "\n",
        "# Optional packages\n",
        "if IN_COLAB:\n",
        "    !pip install -q wandb  # For experiment tracking\n",
        "\n",
        "print(\"‚úÖ Packages installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "testing_header"
      },
      "source": [
        "## üß™ System Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "run_tests"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Running comprehensive test suite...\n",
            "üöÄ AMR Semantic Parsing - Comprehensive Test Suite\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "üìä Testing Data Processing\n",
            "============================================================\n",
            "‚úÖ AMRProcessor initialized\n",
            "‚úÖ DataLoader initialized\n",
            "‚ö†Ô∏è  No processed data found - run data processing first\n",
            "\n",
            "============================================================\n",
            "üî§ Testing Tokenization\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nphuoctho/Documents/LangParse/nlp-semantic-parsing/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ ViT5Tokenizer initialized\n",
            "   Model: VietAI/vit5-base\n",
            "   Vocab size: 36096\n",
            "   Max length: 128\n",
            "‚úÖ Single tokenization:\n",
            "   Input: 'T√¥i y√™u Vi·ªát Nam'\n",
            "   Input tokens: 128\n",
            "   Label tokens: 128\n",
            "‚úÖ Batch tokenization:\n",
            "   Batch size: 3\n",
            "‚úÖ Token decoding works\n",
            "\n",
            "============================================================\n",
            "‚öôÔ∏è  Testing Configuration\n",
            "============================================================\n",
            "‚úÖ Default configuration loaded\n",
            "‚úÖ YAML configuration loaded\n",
            "   Model: VietAI/vit5-base\n",
            "   Batch size: 2\n",
            "   Max samples: 100\n",
            "‚úÖ Configuration validation passed\n",
            "\n",
            "============================================================\n",
            "üìä Testing Evaluation Components\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nphuoctho/Documents/LangParse/nlp-semantic-parsing/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ EvaluationMetrics created\n",
            "   BLEU-4: 0.2\n",
            "   ROUGE-L: 0.55\n",
            "   Exact Match: 0.1\n",
            "‚úÖ Metrics conversion: 9 metrics\n",
            "\n",
            "============================================================\n",
            "üîÆ Testing Inference Components\n",
            "============================================================\n",
            "‚úÖ AMR formatting logic works\n",
            "   Raw: (y / y√™u :ARG0 (t / t√¥i) :ARG1 (v / Vi·ªát_Nam))\n",
            "   Formatted preview: (y / y√™u\n",
            "   :ARG0 (t / t√¥i)\n",
            "   :ARG1 (v / Vi·ªát_Nam...\n",
            "\n",
            "============================================================\n",
            "üíª Testing CLI Interface\n",
            "============================================================\n",
            "‚úÖ main.py imports successfully\n",
            "‚úÖ Argument parser created\n",
            "‚úÖ Help text generated (1485 characters)\n",
            "\n",
            "================================================================================\n",
            "üìä TEST SUMMARY\n",
            "================================================================================\n",
            "‚ùå FAIL     Data Processing\n",
            "‚úÖ PASS     Tokenization\n",
            "‚úÖ PASS     Configuration\n",
            "‚úÖ PASS     Evaluation Components\n",
            "‚úÖ PASS     Inference Components\n",
            "‚úÖ PASS     CLI Interface\n",
            "\n",
            "üéØ Overall: 5/6 tests passed\n",
            "\n",
            "‚ö†Ô∏è  1 tests failed. Check errors above.\n"
          ]
        }
      ],
      "source": [
        "# Run comprehensive tests\n",
        "if Path(\"comprehensive_test.py\").exists():\n",
        "    print(\"üß™ Running comprehensive test suite...\")\n",
        "    exec(open(\"comprehensive_test.py\").read())\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Test file not found, running basic tests...\")\n",
        "\n",
        "    # Basic import tests\n",
        "    try:\n",
        "        from src.utils.config import Config\n",
        "        from src.data_processing import AMRProcessor, DataLoader\n",
        "        from src.tokenization import ViT5Tokenizer\n",
        "\n",
        "        print(\"‚úÖ All core modules imported successfully\")\n",
        "\n",
        "        # Test tokenizer\n",
        "        tokenizer = ViT5Tokenizer(max_length=128)\n",
        "        print(f\"‚úÖ Tokenizer loaded (vocab: {tokenizer.get_vocab_size()})\")\n",
        "\n",
        "        # Test configuration\n",
        "        config = Config()\n",
        "        print(\"‚úÖ Configuration system works\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Basic tests failed: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data_header"
      },
      "source": [
        "## üìä Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "process_data"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÅ Found 2 AMR files in data/train/\n",
            "üîÑ Processing AMR data...\n",
            "‚úÖ Processed 1842 samples\n",
            "   Avg input length: 11.6\n",
            "   Avg output length: 22.7\n",
            "‚úÖ Data split completed:\n",
            "   train: 1473 samples\n",
            "   val: 276 samples\n",
            "   test: 93 samples\n"
          ]
        }
      ],
      "source": [
        "# Check and process data\n",
        "from src.data_processing import AMRProcessor, DataLoader\n",
        "\n",
        "# Check if data exists\n",
        "if Path(\"data/train\").exists():\n",
        "    train_files = list(Path(\"data/train\").glob(\"*.txt\"))\n",
        "    print(f\"üìÅ Found {len(train_files)} AMR files in data/train/\")\n",
        "\n",
        "    if train_files:\n",
        "        # Process data\n",
        "        processor = AMRProcessor()\n",
        "        loader = DataLoader()\n",
        "\n",
        "        print(\"üîÑ Processing AMR data...\")\n",
        "        output_file = processor.process_amr_files(\"data/train\", \"data/processed\")\n",
        "\n",
        "        # Load and show statistics\n",
        "        data = loader.load_jsonl(output_file)\n",
        "        stats = loader.get_data_statistics(data)\n",
        "\n",
        "        print(f\"‚úÖ Processed {len(data)} samples\")\n",
        "        print(f\"   Avg input length: {stats['avg_input_length']:.1f}\")\n",
        "        print(f\"   Avg output length: {stats['avg_output_length']:.1f}\")\n",
        "\n",
        "        # Split data\n",
        "        from src.data_processing.data_loader import DataSplit\n",
        "\n",
        "        split_config = DataSplit(train_ratio=0.8, val_ratio=0.15, test_ratio=0.05)\n",
        "        split_data = loader.split_data(data, split_config)\n",
        "\n",
        "        # Save split data\n",
        "        file_paths = loader.save_split_data(split_data, \"data/processed\")\n",
        "\n",
        "        print(\"‚úÖ Data split completed:\")\n",
        "        for split_name, samples in split_data.items():\n",
        "            print(f\"   {split_name}: {len(samples)} samples\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No .txt files found in data/train/\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  data/train/ directory not found\")\n",
        "    print(\"üí° You can create sample data or upload your AMR files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tokenization_header"
      },
      "source": [
        "## üî§ Tokenization Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "test_tokenization"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üî§ Testing VietAI/vit5-base tokenization...\n",
            "üìä Tokenizer Info:\n",
            "   Model: VietAI/vit5-base\n",
            "   Vocab size: 36,096\n",
            "   Max length: 256\n",
            "\n",
            "üß™ Testing examples:\n",
            "   1. 'T√¥i y√™u Vi·ªát Nam'\n",
            "      Input tokens: 5\n",
            "      Label tokens: 26\n",
            "   2. 'C√¥ ·∫•y ƒëang h·ªçc ti·∫øng Anh'\n",
            "      Input tokens: 7\n",
            "      Label tokens: 39\n",
            "   3. 'H√¥m nay tr·ªùi ƒë·∫πp'\n",
            "      Input tokens: 5\n",
            "      Label tokens: 24\n",
            "\n",
            "‚úÖ Batch tokenization: 3 samples processed\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/nphuoctho/Documents/LangParse/nlp-semantic-parsing/.venv/lib/python3.13/site-packages/transformers/tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Test tokenization with Vietnamese examples\n",
        "from src.tokenization import ViT5Tokenizer\n",
        "\n",
        "print(\"üî§ Testing VietAI/vit5-base tokenization...\")\n",
        "tokenizer = ViT5Tokenizer(max_length=256)\n",
        "\n",
        "# Test examples\n",
        "test_examples = [\n",
        "    {\n",
        "        \"input\": \"T√¥i y√™u Vi·ªát Nam\",\n",
        "        \"output\": \"(y / y√™u :ARG0 (t / t√¥i) :ARG1 (v / Vi·ªát_Nam))\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"C√¥ ·∫•y ƒëang h·ªçc ti·∫øng Anh\",\n",
        "        \"output\": \"(h / h·ªçc :ARG0 (c / c√¥_·∫•y) :ARG1 (t / ti·∫øng_Anh) :aspect (p / progressive))\",\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"H√¥m nay tr·ªùi ƒë·∫πp\",\n",
        "        \"output\": \"(ƒë / ƒë·∫πp :ARG1 (t / tr·ªùi) :time (h / h√¥m_nay))\",\n",
        "    },\n",
        "]\n",
        "\n",
        "print(f\"üìä Tokenizer Info:\")\n",
        "print(f\"   Model: VietAI/vit5-base\")\n",
        "print(f\"   Vocab size: {tokenizer.get_vocab_size():,}\")\n",
        "print(f\"   Max length: {tokenizer.max_length}\")\n",
        "\n",
        "print(f\"\\nüß™ Testing examples:\")\n",
        "for i, example in enumerate(test_examples, 1):\n",
        "    result = tokenizer.tokenize_sample(example[\"input\"], example[\"output\"])\n",
        "    print(f\"   {i}. '{example['input']}'\")\n",
        "    print(\n",
        "        f\"      Input tokens: {len([t for t in result.input_ids if t != tokenizer.tokenizer.pad_token_id])}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"      Label tokens: {len([t for t in result.labels if t != tokenizer.tokenizer.pad_token_id])}\"\n",
        "    )\n",
        "\n",
        "# Test batch processing\n",
        "inputs = [ex[\"input\"] for ex in test_examples]\n",
        "outputs = [ex[\"output\"] for ex in test_examples]\n",
        "batch_results = tokenizer.tokenize_batch(inputs, outputs)\n",
        "\n",
        "print(f\"\\n‚úÖ Batch tokenization: {len(batch_results)} samples processed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config_header"
      },
      "source": [
        "## ‚öôÔ∏è Configuration Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "setup_config"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Configuration created: config/local_config.yaml\n",
            "   Environment: Local (CPU/GPU)\n",
            "   Batch size: 2\n",
            "   Max samples: 100\n",
            "   FP16: False\n",
            "‚úÖ Configuration validation passed\n"
          ]
        }
      ],
      "source": [
        "# Create environment-specific configuration\n",
        "import yaml\n",
        "from src.utils.config import Config\n",
        "\n",
        "# Create configuration based on environment\n",
        "if IN_COLAB:\n",
        "    config_data = {\n",
        "        \"model\": {\n",
        "            \"model_name\": \"VietAI/vit5-base\",\n",
        "            \"max_length\": 512,\n",
        "            \"per_device_train_batch_size\": 8,  # Larger for GPU\n",
        "            \"per_device_eval_batch_size\": 16,\n",
        "            \"learning_rate\": 3e-5,\n",
        "            \"num_train_epochs\": 3,\n",
        "            \"warmup_steps\": 500,\n",
        "            \"fp16\": True,  # Enable for GPU\n",
        "            \"eval_steps\": 250,\n",
        "            \"save_steps\": 250,\n",
        "            \"logging_steps\": 50,\n",
        "        },\n",
        "        \"data\": {\n",
        "            \"train_ratio\": 0.8,\n",
        "            \"val_ratio\": 0.15,\n",
        "            \"test_ratio\": 0.05,\n",
        "            \"max_samples\": None,  # Use all data\n",
        "            \"max_input_length\": 512,\n",
        "            \"max_output_length\": 512,\n",
        "        },\n",
        "        \"training\": {\n",
        "            \"use_wandb\": True,\n",
        "            \"report_to\": \"wandb\",\n",
        "            \"run_name\": \"colab-amr-training\",\n",
        "            \"dataloader_num_workers\": 2,\n",
        "        },\n",
        "        \"paths\": {\n",
        "            \"processed_dir\": \"data/processed\",\n",
        "            \"output_dir\": \"models/amr_colab_model\",\n",
        "            \"log_dir\": \"logs/colab\",\n",
        "        },\n",
        "    }\n",
        "    config_file = \"config/colab_config.yaml\"\n",
        "\n",
        "else:\n",
        "    config_data = {\n",
        "        \"model\": {\n",
        "            \"model_name\": \"VietAI/vit5-base\",\n",
        "            \"max_length\": 256,  # Smaller for local\n",
        "            \"per_device_train_batch_size\": 2,  # Smaller for CPU/limited GPU\n",
        "            \"per_device_eval_batch_size\": 4,\n",
        "            \"learning_rate\": 5e-5,\n",
        "            \"num_train_epochs\": 2,\n",
        "            \"warmup_steps\": 100,\n",
        "            \"fp16\": False,  # Disable for CPU compatibility\n",
        "            \"eval_steps\": 50,\n",
        "            \"save_steps\": 50,\n",
        "            \"logging_steps\": 10,\n",
        "        },\n",
        "        \"data\": {\n",
        "            \"train_ratio\": 0.8,\n",
        "            \"val_ratio\": 0.15,\n",
        "            \"test_ratio\": 0.05,\n",
        "            \"max_samples\": 100,  # Limit for local testing\n",
        "            \"max_input_length\": 256,\n",
        "            \"max_output_length\": 256,\n",
        "        },\n",
        "        \"training\": {\n",
        "            \"use_wandb\": False,\n",
        "            \"report_to\": \"none\",\n",
        "            \"run_name\": \"local-amr-training\",\n",
        "            \"dataloader_num_workers\": 0,\n",
        "        },\n",
        "        \"paths\": {\n",
        "            \"processed_dir\": \"data/processed\",\n",
        "            \"output_dir\": \"models/amr_local_model\",\n",
        "            \"log_dir\": \"logs/local\",\n",
        "        },\n",
        "    }\n",
        "    config_file = \"config/local_config.yaml\"\n",
        "\n",
        "# Save configuration\n",
        "os.makedirs(\"config\", exist_ok=True)\n",
        "with open(config_file, \"w\") as f:\n",
        "    yaml.dump(config_data, f, default_flow_style=False, allow_unicode=True)\n",
        "\n",
        "print(f\"‚úÖ Configuration created: {config_file}\")\n",
        "print(f\"   Environment: {'Colab (GPU)' if IN_COLAB else 'Local (CPU/GPU)'}\")\n",
        "print(f\"   Batch size: {config_data['model']['per_device_train_batch_size']}\")\n",
        "print(f\"   Max samples: {config_data['data']['max_samples'] or 'All'}\")\n",
        "print(f\"   FP16: {config_data['model']['fp16']}\")\n",
        "\n",
        "# Test configuration loading\n",
        "config = Config(config_file=config_file)\n",
        "config.validate()\n",
        "print(\"‚úÖ Configuration validation passed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "training_header"
      },
      "source": [
        "## üöÄ Training (Optional)\n",
        "\n",
        "**Note**: Training may require manual configuration adjustments due to type conversion issues. For production training, use the CLI interface or fix the trainer configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "training_section"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Training Setup\n",
            "‚ö†Ô∏è  Note: Training may require manual configuration fixes\n",
            "üìä Training data: 1473 samples\n",
            "\n",
            "üí° Options:\n",
            "1. Skip training (recommended for testing)\n",
            "2. Attempt training (may fail due to config issues)\n",
            "3. Use CLI training (recommended for production)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üíª For CLI training, use:\n",
            "   python main.py train --config config/local_config.yaml\n"
          ]
        }
      ],
      "source": [
        "# Optional training - may need manual fixes\n",
        "import subprocess\n",
        "\n",
        "print(\"üöÄ Training Setup\")\n",
        "print(\"‚ö†Ô∏è  Note: Training may require manual configuration fixes\")\n",
        "\n",
        "# Check if processed data exists\n",
        "if Path(\"data/processed/train.jsonl\").exists():\n",
        "    train_samples = sum(1 for _ in open(\"data/processed/train.jsonl\"))\n",
        "    print(f\"üìä Training data: {train_samples} samples\")\n",
        "\n",
        "    # Ask user if they want to attempt training\n",
        "    print(\"\\nüí° Options:\")\n",
        "    print(\"1. Skip training (recommended for testing)\")\n",
        "    print(\"2. Attempt training (may fail due to config issues)\")\n",
        "    print(\"3. Use CLI training (recommended for production)\")\n",
        "\n",
        "    choice = input(\"Enter choice (1/2/3): \").strip()\n",
        "\n",
        "    if choice == \"2\":\n",
        "        print(\"üîÑ Attempting training...\")\n",
        "        try:\n",
        "            result = subprocess.run(\n",
        "                [\"python\", \"main.py\", \"train\", \"--config\", config_file],\n",
        "                capture_output=True,\n",
        "                text=True,\n",
        "                timeout=300,\n",
        "            )\n",
        "\n",
        "            if result.returncode == 0:\n",
        "                print(\"‚úÖ Training completed successfully!\")\n",
        "                print(result.stdout)\n",
        "            else:\n",
        "                print(\"‚ùå Training failed:\")\n",
        "                print(result.stderr)\n",
        "\n",
        "        except subprocess.TimeoutExpired:\n",
        "            print(\"‚è∞ Training timeout - this is normal for large datasets\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training error: {e}\")\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        print(\"üíª For CLI training, use:\")\n",
        "        print(f\"   python main.py train --config {config_file}\")\n",
        "\n",
        "    else:\n",
        "        print(\"‚è≠Ô∏è  Skipping training\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå No training data found. Process data first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "summary_header"
      },
      "source": [
        "## üìä Results Summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "summary_section"
      },
      "outputs": [],
      "source": [
        "# Summary of what was accomplished\n",
        "print(\"üéâ AMR Semantic Parsing - Session Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\nüîç Environment: {'Google Colab' if IN_COLAB else 'Local Jupyter'}\")\n",
        "print(f\"üìÅ Working Directory: {os.getcwd()}\")\n",
        "\n",
        "# Check what was accomplished\n",
        "accomplishments = []\n",
        "\n",
        "if Path(\"src\").exists():\n",
        "    accomplishments.append(\"‚úÖ Project structure loaded\")\n",
        "\n",
        "try:\n",
        "    from src.tokenization import ViT5Tokenizer\n",
        "\n",
        "    tokenizer = ViT5Tokenizer(max_length=128)\n",
        "    accomplishments.append(\n",
        "        f\"‚úÖ VietAI/vit5-base tokenizer ({tokenizer.get_vocab_size():,} vocab)\"\n",
        "    )\n",
        "except:\n",
        "    accomplishments.append(\"‚ùå Tokenizer loading failed\")\n",
        "\n",
        "if Path(\"data/processed\").exists():\n",
        "    processed_files = list(Path(\"data/processed\").glob(\"*.jsonl\"))\n",
        "    if processed_files:\n",
        "        total_samples = sum(sum(1 for _ in open(f)) for f in processed_files)\n",
        "        accomplishments.append(f\"‚úÖ Data processing ({total_samples} samples)\")\n",
        "    else:\n",
        "        accomplishments.append(\"‚ö†Ô∏è  Data processing (no JSONL files)\")\n",
        "else:\n",
        "    accomplishments.append(\"‚ùå No processed data\")\n",
        "\n",
        "if Path(config_file).exists():\n",
        "    accomplishments.append(f\"‚úÖ Configuration ({Path(config_file).name})\")\n",
        "\n",
        "if Path(\"models\").exists() and list(Path(\"models\").glob(\"*\")):\n",
        "    accomplishments.append(\"‚úÖ Model training completed\")\n",
        "else:\n",
        "    accomplishments.append(\"‚ö†Ô∏è  No trained models\")\n",
        "\n",
        "print(\"\\nüìã Accomplishments:\")\n",
        "for item in accomplishments:\n",
        "    print(f\"   {item}\")\n",
        "\n",
        "print(\"\\nüéØ Next Steps:\")\n",
        "if IN_COLAB:\n",
        "    print(\"   1. Download results: Files ‚Üí Download folder\")\n",
        "    print(\"   2. Use trained model locally\")\n",
        "    print(\"   3. Continue training with more data\")\n",
        "else:\n",
        "    print(\"   1. Use CLI for production training\")\n",
        "    print(\n",
        "        \"   2. Test with: python main.py predict --model-path <model> --text 'Your text'\"\n",
        "    )\n",
        "    print(\"   3. Interactive mode: python main.py interactive --model-path <model>\")\n",
        "\n",
        "print(\"\\nüìö Documentation:\")\n",
        "print(\"   - README.md: Project overview\")\n",
        "print(\"   - LOCAL_TESTING.md: Local development guide\")\n",
        "print(\"   - python main.py --help: CLI help\")\n",
        "\n",
        "print(\"\\nüéä AMR Semantic Parsing setup complete!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
